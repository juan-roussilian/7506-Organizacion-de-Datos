{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba4b8e-174e-4edc-a7f5-dca0db98d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from preprocessing import limpiar_datos, aplicar_dummy_variables_encoding, eliminar_features, entrenar_iterative_imputer, imputar_missings_iterative\n",
    "from graficos_modelos import mostrar_reporte_clasificacion, graficar_auc_roc,graficar_matriz_confusion\n",
    "from funciones_auxiliares import traer_datasets, traer_dataset_prediccion_final, separar_dataset, encontrar_hiperparametros_RGSCV, mapear_target_binario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608f5146-35a4-4989-9d80-84c5b10061d3",
   "metadata": {},
   "source": [
    "# Arbol de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1bfe73-d902-4602-8a2e-b8497052a0ca",
   "metadata": {},
   "source": [
    "### Obtención de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148b579-0c3a-41b8-b428-3366406e0844",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_sin_target, solo_target = traer_datasets()\n",
    "\n",
    "X_train, X_test, y_train, y_test = separar_dataset(df_sin_target, solo_target)\n",
    "\n",
    "X_train.is_copy=False\n",
    "X_test.is_copy=False\n",
    "y_train.is_copy=False\n",
    "y_test.is_copy=False\n",
    "\n",
    "y_train.set_index('id', inplace=True)\n",
    "y_train = y_train.sort_values(by=['id'], ascending=True).copy()\n",
    "\n",
    "y_test.set_index('id', inplace=True)\n",
    "y_test = y_test.sort_values(by=['id'], ascending=True).copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156b8ef0-0068-4a88-859b-8180a9e4cc03",
   "metadata": {},
   "source": [
    "ESTO DEBERÍA IR EN preprocessing.py !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb0199-0338-4c42-9487-1f4ab5ab6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_continuas = ['id','horas_de_sol','humedad_tarde', 'humedad_temprano', 'mm_evaporados_agua', 'mm_lluvia_dia', 'nubosidad_tarde', 'nubosidad_temprano', 'presion_atmosferica_tarde', 'presion_atmosferica_temprano', 'rafaga_viento_max_velocidad','temp_max', 'temp_min', 'temperatura_tarde', 'temperatura_temprano',  'velocidad_viendo_tarde','velocidad_viendo_temprano']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ceb50-56ef-4c89-8247-00960d31dbca",
   "metadata": {},
   "source": [
    "### Definiendo distintos preprocesamientos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c07afed-7652-4936-8ac5-e53694eee262",
   "metadata": {},
   "source": [
    "Definiremos entonces dos preprocesamientos distintos a comparar para este modelo, y quedarnos con el mejor de ellos cuando probemos en holdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c87175-226e-49c3-9bb2-fb77ba05ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesamiento_1(df_original:pd.DataFrame, imputer=None):\n",
    "    df = df_original.copy(deep=True)\n",
    "    df = limpiar_datos(df)\n",
    "    df = aplicar_dummy_variables_encoding(df, ['llovieron_hamburguesas_hoy'])\n",
    "    eliminar_features(df, ['dia','barrio', 'direccion_viento_tarde', 'direccion_viento_temprano', 'rafaga_viento_max_direccion'])\n",
    "    \n",
    "    if(imputer is None):\n",
    "        imputer = entrenar_iterative_imputer(df)\n",
    "    df = imputar_missings_iterative(df, imputer)\n",
    "    \n",
    "    return df, imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a64b45-b16f-40e0-82ab-974b3478ff6c",
   "metadata": {},
   "source": [
    "Vemos que el primero de ellos primero corrije los datos con formato erróneo del dataset, como por ejemplo direcciones IP registradas en la feature 'presion_atmosférica_tarde', o datos que deberían ser NaNs pero son whitespace. Luego se aplica dummy encoding a la única feature categórica que nos resultó importante del análisis en el TP1. Pasamos a eliminar el resto de features categóricas y finalmente completamos los missings con el IterativeImputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d1cc53-8987-4dd1-8cc7-6e2708ee3a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesamiento_2(df_original:pd.DataFrame, imputer=None, seleccion_random_forest=None):\n",
    "    df = df_original.copy(deep=True)\n",
    "    df = limpiar_datos(df)\n",
    "    df = aplicar_dummy_variables_encoding(df, ['llovieron_hamburguesas_hoy', 'dia','barrio', 'direccion_viento_tarde', 'direccion_viento_temprano', 'rafaga_viento_max_direccion'])\n",
    "    \n",
    "    if(imputer == None):\n",
    "        imputer = entrenar_iterative_imputer(df)\n",
    "    df = imputar_missings_iterative(df, imputer)\n",
    "    \n",
    "    if(seleccion_random_forest is None):\n",
    "        sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
    "        sel.fit(df, y_train.to_numpy().ravel())\n",
    "        seleccion_random_forest= df.columns[(sel.get_support())]\n",
    "    \n",
    "    eliminar_features(df, df.columns.difference(seleccion_random_forest))\n",
    "    \n",
    "    return df, imputer, seleccion_random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e9ab4-0b8d-4a89-84c3-77492ef25b89",
   "metadata": {},
   "source": [
    "El segundo preprocesamiento primero corrije los datos con formato erróneo del dataset, como por ejemplo direcciones IP registradas en la feature 'presion_atmosférica_tarde', o datos que deberían ser NaNs pero son whitespace. Luego se aplica dummy encoding a todas las features categóricas. Una vez hecho esto, completamos los missings con el IterativeImputer para poder utilizar un clasificador Random Forest como método de selección de atributos. Gracias a esta selección, podemos droppear del dataframes aquellas features que RandomForest no haya considerado importante para terminar el preprocesamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bcf2a7-b997-4204-bb99-b84b48dd6cdc",
   "metadata": {},
   "source": [
    "### Aplicamos preprocesamientos 1 y 2 para obtener X_train_1 / X_test_1 y X_train_2 / X_test_2 respectivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1536d8-12f0-4db2-8ef3-5e55ab9efea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, imputer_entrenado_1 = preprocesamiento_1(X_train)\n",
    "X_test_1, imputer_entrenado_1 = preprocesamiento_1(X_test, imputer_entrenado_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb8671-d6ef-4e87-a0a7-88ba1bf53be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, imputer_entrenado_2, seleccion_RF = preprocesamiento_2(X_train)\n",
    "X_test_2, imputer_entrenado_2, seleccion_RF = preprocesamiento_2(X_test, imputer_entrenado_2, seleccion_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8d8c9-e725-4fa4-ae4c-2d034f6e3058",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Búsqueda de Hiperparámetros para el modelo que entrenará sobre X_train_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395a8e07-17c6-40f3-8868-d95363332ddb",
   "metadata": {},
   "source": [
    "Buscamos hiperparámetros con RandomGridSearch, pues GridSearch iterativamente es bastante lento.\n",
    "Estos parámetros serán:\n",
    "* criterion: La función que evalua la calidad de una división en el árbol\n",
    "* min_samples_leaf: La mínima cantidad de muestras requeridas para dividir un nodo\n",
    "* max_depth: La máxima profundiad que le permitimos al árbol. Un árbol muy profundo implicaría un overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16bf2c9-0606-421d-9959-0e14bf33c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'criterion': ['gini','entropy'], 'min_samples_leaf':np.arange(1,16),'max_depth': np.arange(1,30)}\n",
    "hiperparametros = encontrar_hiperparametros_RGSCV(DecisionTreeClassifier(), params=params, x_np=X_train_1.to_numpy(), y_np=y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10c8a2-9d3a-4ffd-a2f2-38d51a0ae6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterio_elegido_1 = hiperparametros['criterion']\n",
    "profundidad_elegida_1 = hiperparametros['max_depth']\n",
    "min_leaf_elegido_1 = hiperparametros['min_samples_leaf']\n",
    "print(f'Mejor criterio: {criterio_elegido_1}')\n",
    "print(f'Mejor profundidad máxima: {profundidad_elegida_1}')\n",
    "print(f'Mejor mínima cantidad de instancias por hoja: {min_leaf_elegido_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5342653-7674-4ac6-990c-d79390cae5d6",
   "metadata": {},
   "source": [
    "### Entrenando el Modelo 1 sobre X_train_1 con CrossValidation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c86964-0d50-455b-acb9-8ea17a09a2c5",
   "metadata": {},
   "source": [
    "Procedemos a testear con kfolds, stratificados pues nuestro dataset es desbalanceado. Además, usamos los hiperparámetros encontrados previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c2f2e1-9415-455d-a3ae-7476df162242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5)\n",
    "for fold_idx, (train_index, test_index) in enumerate(kf.split(X_train_1, y_train)):\n",
    "    arbol_clasificacion_1 = DecisionTreeClassifier(max_depth=profundidad_elegida_1, min_samples_leaf=min_leaf_elegido_1, criterion=criterio_elegido_1, random_state=117)\n",
    "    arbol_clasificacion_1.fit(X_train_1.iloc[train_index], y_train.iloc[train_index])\n",
    "    print ('Reporte para el FOLD ' + str(fold_idx))\n",
    "    print(classification_report(y_train.iloc[test_index], arbol_clasificacion_1.predict(X_train_1.iloc[test_index]), target_names=['No llueven hamburguesas al dia siguiente', 'Llueven hamburguesas al dia siguiente']))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd665d-2fb2-4334-89ae-11907e807abb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Búsqueda de Hiperparámetros para el modelo que entrenará sobre X_train_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbe437c-946f-4ae5-8a7d-16bb6313d162",
   "metadata": {},
   "source": [
    "Buscamos hiperparámetros con RandomGridSearch, pues GridSearch iterativamente es bastante lento.\n",
    "Estos parámetros serán:\n",
    "* criterion: La función que evalua la calidad de una división en el árbol\n",
    "* min_samples_leaf: La mínima cantidad de muestras requeridas para dividir un nodo\n",
    "* max_depth: La máxima profundiad que le permitimos al árbol. Un árbol muy profundo implicaría un overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffea2c9-4f90-4afa-9433-f2c13617a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'criterion': ['gini','entropy'], 'min_samples_leaf':np.arange(1,16),'max_depth': np.arange(1,30)}\n",
    "hiperparametros = encontrar_hiperparametros_RGSCV(DecisionTreeClassifier(), params=params, x_np=X_train_2.to_numpy(), y_np=y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dde1140-6441-48de-82e1-afb16e450895",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterio_elegido_2 = hiperparametros['criterion']\n",
    "profundidad_elegida_2 = hiperparametros['max_depth']\n",
    "min_leaf_elegido_2 = hiperparametros['min_samples_leaf']\n",
    "print(f'Mejor criterio: {criterio_elegido_2}')\n",
    "print(f'Mejor profundidad máxima: {profundidad_elegida_2}')\n",
    "print(f'Mejor mínima cantidad de instancias por hoja: {min_leaf_elegido_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2765f65-bee9-4440-aad4-0fd7b3982bad",
   "metadata": {},
   "source": [
    "### Entrenando el Modelo 2 sobre X_train_2 con CrossValidation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deb7530-76c1-44dc-abb5-90f6e9d42b12",
   "metadata": {},
   "source": [
    "Procedemos a testear con kfolds, stratificados pues nuestro dataset es desbalanceado. Además, usamos los hiperparámetros encontrados previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b26e0f3-a710-4a9f-90e7-4d6f61be92e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5)\n",
    "for fold_idx, (train_index, test_index) in enumerate(kf.split(X_train_2, y_train)):\n",
    "    arbol_clasificacion_2 = DecisionTreeClassifier(max_depth=profundidad_elegida_2, min_samples_leaf=min_leaf_elegido_2, criterion=criterio_elegido_2, random_state=117)\n",
    "    arbol_clasificacion_2.fit(X_train_2.iloc[train_index], y_train.iloc[train_index])\n",
    "    print ('Reporte para el FOLD ' + str(fold_idx))\n",
    "    print(classification_report(y_train.iloc[test_index], arbol_clasificacion_2.predict(X_train_2.iloc[test_index]), target_names=['No llueven hamburguesas al dia siguiente', 'Llueven hamburguesas al dia siguiente']))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f454599-13d2-4547-b906-fc0c15baa62c",
   "metadata": {},
   "source": [
    "### Predicción del modelo sobre holdout con el modelo que mejor resultó al hacer CrossValidation: Modelo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f16703-38ae-49fa-986c-1ddfecef41b8",
   "metadata": {},
   "source": [
    "Una vez entrenado y validado el modelo con CrossValidation, viendo que las métricas resultantes fueron razonables y no dan indicio de overfit o mala división del dataset, pasaremos a probar el modelo en la partición Holdout. Para esto usaremos los mejores hiperparámetros hallados previamente en Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe0a9f-961c-487d-8548-5e0bbafc7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol_holdout = DecisionTreeClassifier(max_depth=profundidad_elegida_1, min_samples_leaf=min_leaf_elegido_1, criterion=criterio_elegido_1, random_state=117)\n",
    "arbol_holdout.fit(X_train_1.to_numpy(), y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da92c3-cf58-431e-9592-f6df4337ae2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapeo_binario_v = np.vectorize(mapear_target_binario)\n",
    "y_pred = arbol_holdout.predict(X_test_1.to_numpy())\n",
    "y_pred_binario = mapeo_binario_v(y_pred)\n",
    "y_pred_proba = arbol_holdout.predict_proba(X_test_1.to_numpy())[:, 1]\n",
    "y_test_binario = y_test['llovieron_hamburguesas_al_dia_siguiente'].map({'si': 1, 'no': 0}).to_numpy()\n",
    "print(classification_report(y_test['llovieron_hamburguesas_al_dia_siguiente'].to_numpy(), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c1241-57e9-4aad-9fe5-7ec16424ccf7",
   "metadata": {},
   "source": [
    "### Curva AUC ROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a29849-6959-4db6-ac1e-b34f9f3fd8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_auc_roc(y_test_binario, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d1f2cc-3d4a-4b92-9d39-ea3988da6627",
   "metadata": {},
   "source": [
    "### Matriz de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e1bcf-6980-475f-bba0-73c6780b898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_matriz_confusion(y_test_binario, y_pred_binario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a427a2-52ff-4141-ba6d-d08072191805",
   "metadata": {},
   "source": [
    "## Predicción con el dataset nuevo\n",
    "A continuación, realizamos la predicción con el árbol sobre el dataset de predicciones nuevo, y escribimos los resultados al archivo 'ArbolDeDecision.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01fc36b-d7fb-4052-94d3-3f2edca94f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from funciones_auxiliares import exportar_prediccion_final\n",
    "\n",
    "df_prediccion_final = traer_dataset_prediccion_final()\n",
    "ids = df_prediccion_final['id'].to_numpy()\n",
    "\n",
    "df_prediccion_final, imputer_entrenado_1 = preprocesamiento_1(df_prediccion_final, imputer_entrenado_1)\n",
    "predicciones = arbol_holdout.predict(df_prediccion_final.to_numpy())\n",
    "\n",
    "exportar_prediccion_final(ids, predicciones, 'arbol_de_decision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53635bdc-ddda-4488-889c-7e0bc44c32e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
