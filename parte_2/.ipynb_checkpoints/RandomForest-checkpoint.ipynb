{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcba4b8e-174e-4edc-a7f5-dca0db98d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from preprocessing import limpiar_datos, aplicar_dummy_variables_encoding, eliminar_features, entrenar_iterative_imputer, imputar_missings_iterative, aplicar_hashing_trick\n",
    "from graficos_modelos import mostrar_reporte_clasificacion, graficar_auc_roc,graficar_matriz_confusion\n",
    "from funciones_auxiliares import traer_datasets, traer_dataset_prediccion_final, separar_dataset, encontrar_hiperparametros_RGSCV, mapear_target_binario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608f5146-35a4-4989-9d80-84c5b10061d3",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1bfe73-d902-4602-8a2e-b8497052a0ca",
   "metadata": {},
   "source": [
    "### Obtención de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3148b579-0c3a-41b8-b428-3366406e0844",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_sin_target, solo_target = traer_datasets()\n",
    "\n",
    "X_train, X_test, y_train, y_test = separar_dataset(df_sin_target, solo_target)\n",
    "\n",
    "X_train.is_copy=False\n",
    "X_test.is_copy=False\n",
    "y_train.is_copy=False\n",
    "y_test.is_copy=False\n",
    "\n",
    "y_train.set_index('id', inplace=True)\n",
    "y_train = y_train.sort_values(by=['id'], ascending=True).copy()\n",
    "\n",
    "y_test.set_index('id', inplace=True)\n",
    "y_test = y_test.sort_values(by=['id'], ascending=True).copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d37c8-1d37-455b-aaab-6cf5e39e03a7",
   "metadata": {},
   "source": [
    "### Definiendo distintos preprocesamientos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c9ca7d-482f-4e48-8624-2e8cc5ae3d42",
   "metadata": {},
   "source": [
    "Definiremos entonces dos preprocesamientos distintos a comparar para este modelo, y quedarnos con el mejor de ellos cuando probemos en holdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3977e322-6799-4aba-8532-0c214c2a047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesamiento_1(df_original:pd.DataFrame, imputer=None):\n",
    "    df = df_original.copy(deep=True)\n",
    "    df = limpiar_datos(df)\n",
    "    df = aplicar_hashing_trick(df, ['llovieron_hamburguesas_hoy', 'rafaga_viento_max_direccion'], [1,4])\n",
    "    eliminar_features(df, ['dia','barrio', 'direccion_viento_tarde', 'direccion_viento_temprano'])\n",
    "    \n",
    "    if(imputer is None):\n",
    "        imputer = entrenar_iterative_imputer(df)\n",
    "    df = imputar_missings_iterative(df, imputer)\n",
    "    \n",
    "    return df, imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4785efd-8a66-4012-9fa3-c23e674b59e4",
   "metadata": {},
   "source": [
    "Vemos que el primero de ellos primero corrije los datos con formato erróneo del dataset, como por ejemplo direcciones IP registradas en la feature 'presion_atmosférica_tarde', o datos que deberían ser NaNs pero son whitespace. Luego aplicaremos un hashing trick para llovieron_hamburguesas_hoy la cual fue una feature de interés en la primera parte del trabajo práctico. Como prueba adicional en este preprocesamiento incluiremos rafaga_viento_max_direccion a las features categóricas que convertirá el hashing trick. Pasamos a eliminar el resto de features categóricas y finalmente completamos los missings con el IterativeImputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ebc5ab5-86bd-4998-8ee3-4b7baeb35a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesamiento_2(df_original:pd.DataFrame, imputer=None, seleccion_random_forest=None):\n",
    "    df = df_original.copy(deep=True)\n",
    "    df = limpiar_datos(df)\n",
    "    eliminar_features(df, ['dia'])\n",
    "    df = aplicar_dummy_variables_encoding(df, ['llovieron_hamburguesas_hoy','barrio', 'direccion_viento_tarde', 'direccion_viento_temprano', 'rafaga_viento_max_direccion'])\n",
    "    if(imputer == None):\n",
    "        imputer = entrenar_iterative_imputer(df)\n",
    "    df = imputar_missings_iterative(df, imputer)\n",
    "    \n",
    "    if(seleccion_random_forest is None):\n",
    "        sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
    "        sel.fit(df, y_train.to_numpy().ravel())\n",
    "        seleccion_random_forest= df.columns[(sel.get_support())]\n",
    "    \n",
    "    eliminar_features(df, df.columns.difference(seleccion_random_forest))\n",
    "    \n",
    "    return df, imputer, seleccion_random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32670b87-3f5a-4dea-9f9e-bc05aa629049",
   "metadata": {},
   "source": [
    "El segundo preprocesamiento primero corrije los datos con formato erróneo del dataset, como por ejemplo direcciones IP registradas en la feature 'presion_atmosférica_tarde', o datos que deberían ser NaNs pero son whitespace. Luego se aplica dummy encoding a todas las features categóricas menos el día, que tiene muy alta cardinalidad. Una vez hecho esto, completamos los missings con el IterativeImputer para poder utilizar un clasificador Random Forest como método de selección de atributos. Si bien suena redundante entrenar un RandomForest para seleccionar las features de otro RandomForest, el primero de ellos tendrá un número mucho menor de estimadores y la mayoría de sus hiperparámetros por defecto. Gracias a esta selección, podemos droppear del dataframes aquellas features que RandomForest auxiliar no haya considerado importante para terminar el preprocesamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a01a14d-94aa-4577-8dc3-7fbf01c1c3ec",
   "metadata": {},
   "source": [
    "### Aplicamos preprocesamientos 1 y 2 para obtener X_train_1 / X_test_1 y X_train_2 / X_test_2 respectivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58b07bfd-875b-45da-b773-a6c274292f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, imputer_entrenado_1 = preprocesamiento_1(X_train)\n",
    "X_test_1, imputer_entrenado_1 = preprocesamiento_1(X_test, imputer_entrenado_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341c6299-991e-45b9-b7bf-1a6b4ad26305",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, imputer_entrenado_2, seleccion_RF = preprocesamiento_2(X_train)\n",
    "X_test_2, imputer_entrenado_2, seleccion_RF = preprocesamiento_2(X_test, imputer_entrenado_2, seleccion_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6679591-ab98-48a1-a969-eb16b92cf574",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Búsqueda de Hiperparámetros para el modelo que entrenará sobre X_train_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395a8e07-17c6-40f3-8868-d95363332ddb",
   "metadata": {},
   "source": [
    "Buscamos hiperparámetros con RandomGridSearch, pues GridSearch iterativamente es bastante lento. Para RandomForest, estos hiperparámetros serán:\n",
    "* criterion: La función para medir la calidad de una división.\n",
    "* n_estimators: La cantidad de árboles en el bosque.\n",
    "* max_ depth: La profundidad máxima del árbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16bf2c9-0606-421d-9959-0e14bf33c8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guillermo\\anaconda3\\envs\\orgadatos\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 54 is smaller than n_iter=100. Running 54 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "params = {'criterion': ['gini','entropy'], 'n_estimators':[100,300,500],'max_depth': np.arange(3,12)}\n",
    "hiperparametros = encontrar_hiperparametros_RGSCV(RandomForestClassifier(), params=params, x_np=X_train_1.to_numpy(), y_np=y_train.to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10c8a2-9d3a-4ffd-a2f2-38d51a0ae6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterio_elegido_1 = hiperparametros['criterion']\n",
    "cantidad_estimadores_elegida_1 = hiperparametros['n_estimators']\n",
    "profundidad_elegida_1 = hiperparametros['max_depth']\n",
    "print(f'Mejor criterio: {criterio_elegido_1}')\n",
    "print(f'Mejor cantidad de estimadores: {cantidad_estimadores_elegida_1}')\n",
    "print(f'Mejor profundidad_maxima: {profundidad_elegida_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4fdf3a-cdb9-4d61-a6e1-d638ccdf2f25",
   "metadata": {},
   "source": [
    "### Entrenando el Modelo 1 sobre X_train_1 con CrossValidation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c86964-0d50-455b-acb9-8ea17a09a2c5",
   "metadata": {},
   "source": [
    "Procedemos a testear con kfolds, stratificados pues nuestro dataset es desbalanceado. Además, usamos los hiperparámetros encontrados previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c2f2e1-9415-455d-a3ae-7476df162242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5)\n",
    "for fold_idx, (train_index, test_index) in enumerate(kf.split(X_train_1, y_train)):\n",
    "    bosque_clasificacion = RandomForestClassifier(max_depth=profundidad_elegida_1, n_estimators=cantidad_estimadores_elegida_1, criterion=criterio_elegido_1)\n",
    "    bosque_clasificacion.fit(X_train_1.iloc[train_index], y_train.iloc[train_index].values.ravel())\n",
    "    print ('Reporte para el FOLD ' + str(fold_idx))\n",
    "    print(classification_report(y_train.iloc[test_index], bosque_clasificacion.predict(X_train_1.iloc[test_index]), target_names=['No llueven hamburguesas al dia siguiente', 'Llueven hamburguesas al dia siguiente']))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315bfd0b-302f-4205-bc77-b13f6ba43043",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Búsqueda de Hiperparámetros para el modelo que entrenará sobre X_train_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa8ad3-9a0e-455e-82cb-1621f69f3752",
   "metadata": {},
   "source": [
    "Buscamos hiperparámetros con RandomGridSearch, pues GridSearch iterativamente es bastante lento. Para RandomForest, estos hiperparámetros serán:\n",
    "* criterion: La función para medir la calidad de una división.\n",
    "* n_estimators: La cantidad de árboles en el bosque.\n",
    "* max_ depth: La profundidad máxima del árbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9729e3-1081-42b2-b234-29920eba0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'criterion': ['gini','entropy'], 'n_estimators':[100,300,500],'max_depth': np.arange(3,12)}\n",
    "hiperparametros = encontrar_hiperparametros_RGSCV(RandomForestClassifier(), params=params, x_np=X_train_2.to_numpy(), y_np=y_train.to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10b110-95b6-44c5-bd1a-d78a84d2a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterio_elegido_2 = hiperparametros['criterion']\n",
    "cantidad_estimadores_elegida_2 = hiperparametros['n_estimators']\n",
    "profundidad_elegida_2 = hiperparametros['max_depth']\n",
    "print(f'Mejor criterio: {criterio_elegido_2}')\n",
    "print(f'Mejor cantidad de estimadores: {cantidad_estimadores_elegida_2}')\n",
    "print(f'Mejor profundidad_maxima: {profundidad_elegida_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c5c5cc-dcef-46ba-9b4d-a074e78c1df0",
   "metadata": {},
   "source": [
    "### Entrenando el Modelo 2 sobre X_train_2 con CrossValidation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b164277-8b2a-40eb-9d6f-233e59fa8f2a",
   "metadata": {},
   "source": [
    "Procedemos a testear con kfolds, stratificados pues nuestro dataset es desbalanceado. Además, usamos los hiperparámetros encontrados previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e548154-7371-400d-b65d-2bf5ddb1fee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5)\n",
    "for fold_idx, (train_index, test_index) in enumerate(kf.split(X_train_2, y_train)):\n",
    "    bosque_clasificacion = RandomForestClassifier(max_depth=profundidad_elegida_2, n_estimators=cantidad_estimadores_elegida_2, criterion=criterio_elegido_2)\n",
    "    bosque_clasificacion.fit(X_train_2.iloc[train_index], y_train.iloc[train_index].values.ravel())\n",
    "    print ('Reporte para el FOLD ' + str(fold_idx))\n",
    "    print(classification_report(y_train.iloc[test_index], bosque_clasificacion.predict(X_train_2.iloc[test_index]), target_names=['No llueven hamburguesas al dia siguiente', 'Llueven hamburguesas al dia siguiente']))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f454599-13d2-4547-b906-fc0c15baa62c",
   "metadata": {},
   "source": [
    "### Predicción del modelo sobre holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f16703-38ae-49fa-986c-1ddfecef41b8",
   "metadata": {},
   "source": [
    "Una vez entrenado y validado el modelo con CrossValidation, viendo que las métricas resultantes fueron razonables y no dan indicio de overfit o mala división del dataset, pasaremos a probar el modelo que en mejores métricas resultó durante Crossvalidation, en la partición Holdout. Para esto usaremos los mejores hiperparámetros hallados previamente en Training para el respectivo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe0a9f-961c-487d-8548-5e0bbafc7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "bosque_holdout = RandomForestClassifier(max_depth=profundidad_elegida, n_estimators=cantidad_estimadores_elegida, criterion=criterio_elegido)\n",
    "bosque_holdout.fit(X_train.to_numpy(), y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da92c3-cf58-431e-9592-f6df4337ae2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapeo_binario_v = np.vectorize(mapear_target_binario)\n",
    "y_pred = bosque_holdout.predict(X_test.to_numpy())\n",
    "y_pred_binario = mapeo_binario_v(y_pred)\n",
    "y_pred_proba = bosque_holdout.predict_proba(X_test.to_numpy())[:, 1]\n",
    "y_test_binario = y_test['llovieron_hamburguesas_al_dia_siguiente'].map({'si': 1, 'no': 0}).to_numpy()\n",
    "print(classification_report(y_test['llovieron_hamburguesas_al_dia_siguiente'].to_numpy(), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c1241-57e9-4aad-9fe5-7ec16424ccf7",
   "metadata": {},
   "source": [
    "### Curva AUC ROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a29849-6959-4db6-ac1e-b34f9f3fd8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_auc_roc(y_test_binario, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d1f2cc-3d4a-4b92-9d39-ea3988da6627",
   "metadata": {},
   "source": [
    "### Matriz de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e1bcf-6980-475f-bba0-73c6780b898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_matriz_confusion(y_test_binario, y_pred_binario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a427a2-52ff-4141-ba6d-d08072191805",
   "metadata": {},
   "source": [
    "## Predicción con el dataset nuevo\n",
    "A continuación, realizamos la predicción con el RandomForest sobre el dataset de predicciones nuevo, y la escribimos al archivo 'RandomForest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01fc36b-d7fb-4052-94d3-3f2edca94f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from funciones_auxiliares import exportar_prediccion_final\n",
    "\n",
    "df_prediccion_final = traer_dataset_prediccion_final()\n",
    "ids = df_prediccion_final['id'].to_numpy()\n",
    "\n",
    "df_prediccion_final = preprocesamiento_basico([df_prediccion_final])[0]\n",
    "predicciones = bosque_holdout.predict(df_prediccion_final.to_numpy())\n",
    "\n",
    "exportar_prediccion_final(ids, predicciones, 'RandomForest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
